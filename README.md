# SciFi Book QA Explorer

A Python project that lets you ask questions about a collection of science fiction books using modern LLM and vector search technology.

## Features
- Loads and indexes a folder of sci-fi book `.txt` files
- Uses OpenAI embeddings and FAISS for semantic search
- Retrieves relevant book passages to answer user questions
- Answers are generated by a GPT-4-turbo model, strictly based on retrieved context
- Logging and persistent vector store for efficient repeated queries

## Project Structure
```
mini-project-1/
├── data/                # Folder with .txt book files
├── db/faiss_index/      # FAISS vector store (auto-generated)
├── logs/                # Log files
├── src/mini_project_1/  # Main source code
│   └── project.py       # Main logic
├── tests/               # Unit tests
├── requirements.txt     # Python dependencies
├── pyproject.toml       # Poetry config
└── README.md            # This file
```

## Setup
1. **Clone the repo and install dependencies**
   ```sh
   git clone <repo-url>
   cd mini-project-1
   pip install -r requirements.txt
   ```
2. **Set up your OpenAI API credentials**
   - Create a `.env` file in the project root with:
     ```
     OPENAI_API_KEY=your-key-here
     OPENAI_ENDPOINT=https://api.openai.com/v1
     ```
3. **Add your `.txt` books to the `data/` folder**

## Usage
Run the main script:
```sh
python -m src.mini_project_1.project
```
You will be prompted to enter questions about the books. The system will retrieve relevant passages and answer using the LLM.

## Testing
Run unit tests with:
```sh
pytest
```

## Dependencies
- Python 3.13+
- dotenv
- langchain, langchain-openai, langchain-community
- faiss-cpu
- pytest

## Notes
- Answers are strictly based on the retrieved context. If the answer is not in the books, the model will say "I don't know."
- The vector store is persisted in `db/faiss_index/` for fast repeated queries.


